{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# LSTM vs Transformer: Sentiment Analysis Comparison\n",
                "\n",
                "## Week 3 - Understanding How Different Models Read Text\n",
                "\n",
                "### What You'll Learn:\n",
                "1. **LSTM Model** - Sequential text processing\n",
                "2. **Transformer Model** - Parallel attention-based processing\n",
                "3. **Direct Comparison** - Performance, speed, and understanding\n",
                "4. **Why Transformers Win** - Understanding the advantages\n",
                "5. **üéØ Practical**: Build and compare both models\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. The Challenge: Sentiment Analysis\n",
                "\n",
                "### Task:\n",
                "Given a movie review, classify it as **Positive** or **Negative**.\n",
                "\n",
                "**Examples:**\n",
                "```\n",
                "\"This movie was absolutely fantastic!\" ‚Üí Positive ‚úÖ\n",
                "\"Terrible waste of time and money.\" ‚Üí Negative ‚ùå\n",
                "```\n",
                "\n",
                "### Two Approaches:\n",
                "\n",
                "**LSTM (Sequential):**\n",
                "```\n",
                "Word 1 ‚Üí Word 2 ‚Üí Word 3 ‚Üí Word 4 ‚Üí Prediction\n",
                "  ‚Üì        ‚Üì        ‚Üì        ‚Üì\n",
                "Processes one word at a time, passing information forward\n",
                "```\n",
                "\n",
                "**Transformer (Parallel):**\n",
                "```\n",
                "Word 1 ‚îÄ‚îÄ‚îê\n",
                "Word 2 ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚Üí All words processed together!\n",
                "Word 3 ‚îÄ‚îÄ‚î§    Each word \"attends\" to all others\n",
                "Word 4 ‚îÄ‚îÄ‚îò\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install required packages (uncomment if needed)\n",
                "# !pip install torch transformers datasets scikit-learn matplotlib numpy tqdm"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "from transformers import AutoTokenizer\n",
                "from datasets import load_dataset\n",
                "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
                "import time\n",
                "from tqdm import tqdm\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Set random seeds for reproducibility\n",
                "torch.manual_seed(42)\n",
                "np.random.seed(42)\n",
                "\n",
                "# Check for GPU\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f\"Using device: {device}\")\n",
                "print(f\"PyTorch version: {torch.__version__}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Load and Prepare Data\n",
                "\n",
                "We'll use the IMDB movie review dataset:\n",
                "- **50,000 reviews** total\n",
                "- **25,000 for training**, 25,000 for testing\n",
                "- **Balanced**: 50% positive, 50% negative\n",
                "\n",
                "For this tutorial, we'll use a smaller subset for faster training."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load IMDB dataset\n",
                "print(\"Loading IMDB dataset...\")\n",
                "train_dataset = load_dataset(\"imdb\", split=\"train[:5000]\")  # 5000 samples for training\n",
                "test_dataset = load_dataset(\"imdb\", split=\"test[:1000]\")    # 1000 samples for testing\n",
                "\n",
                "print(f\"\\nTraining samples: {len(train_dataset)}\")\n",
                "print(f\"Test samples: {len(test_dataset)}\")\n",
                "\n",
                "# Show example\n",
                "print(f\"\\n{'='*60}\")\n",
                "print(\"Example Review:\")\n",
                "print(f\"{'='*60}\")\n",
                "example = train_dataset[0]\n",
                "print(f\"Text: {example['text'][:200]}...\")\n",
                "print(f\"\\nLabel: {example['label']} ({'Positive' if example['label'] == 1 else 'Negative'})\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Text Preprocessing\n",
                "\n",
                "### Understanding Tokenization:\n",
                "\n",
                "**What is tokenization?**\n",
                "Converting text into numbers that models can understand.\n",
                "\n",
                "```\n",
                "Text:   \"This movie is great!\"\n",
                "         ‚Üì\n",
                "Tokens: [\"This\", \"movie\", \"is\", \"great\", \"!\"]\n",
                "         ‚Üì\n",
                "IDs:    [2023, 3185, 2003, 2307, 999]\n",
                "```\n",
                "\n",
                "We'll create a simple vocabulary-based tokenizer for LSTM and use BERT tokenizer for Transformer."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Simple tokenizer for LSTM\n",
                "class SimpleTokenizer:\n",
                "    def __init__(self, max_vocab_size=10000):\n",
                "        self.max_vocab_size = max_vocab_size\n",
                "        self.word_to_idx = {}\n",
                "        self.idx_to_word = {}\n",
                "        \n",
                "    def build_vocab(self, texts):\n",
                "        \"\"\"Build vocabulary from texts.\"\"\"\n",
                "        word_freq = {}\n",
                "        \n",
                "        # Count word frequencies\n",
                "        for text in texts:\n",
                "            for word in text.lower().split():\n",
                "                word_freq[word] = word_freq.get(word, 0) + 1\n",
                "        \n",
                "        # Sort by frequency and take top words\n",
                "        sorted_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)\n",
                "        \n",
                "        # Reserve 0 for padding, 1 for unknown\n",
                "        self.word_to_idx = {'<PAD>': 0, '<UNK>': 1}\n",
                "        self.idx_to_word = {0: '<PAD>', 1: '<UNK>'}\n",
                "        \n",
                "        # Add top words\n",
                "        for idx, (word, _) in enumerate(sorted_words[:self.max_vocab_size - 2], start=2):\n",
                "            self.word_to_idx[word] = idx\n",
                "            self.idx_to_word[idx] = word\n",
                "        \n",
                "        print(f\"Vocabulary size: {len(self.word_to_idx)}\")\n",
                "    \n",
                "    def encode(self, text, max_length=256):\n",
                "        \"\"\"Convert text to sequence of indices.\"\"\"\n",
                "        words = text.lower().split()\n",
                "        indices = [self.word_to_idx.get(word, 1) for word in words]  # 1 is <UNK>\n",
                "        \n",
                "        # Pad or truncate\n",
                "        if len(indices) < max_length:\n",
                "            indices = indices + [0] * (max_length - len(indices))  # 0 is <PAD>\n",
                "        else:\n",
                "            indices = indices[:max_length]\n",
                "        \n",
                "        return indices\n",
                "\n",
                "# Build vocabulary from training data\n",
                "simple_tokenizer = SimpleTokenizer(max_vocab_size=10000)\n",
                "simple_tokenizer.build_vocab([item['text'] for item in train_dataset])\n",
                "\n",
                "# Test tokenization\n",
                "test_text = \"This movie is absolutely fantastic!\"\n",
                "encoded = simple_tokenizer.encode(test_text, max_length=20)\n",
                "print(f\"\\nExample tokenization:\")\n",
                "print(f\"Text: {test_text}\")\n",
                "print(f\"Encoded: {encoded[:10]}...\")  # Show first 10 tokens"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Prepare datasets for both models\n",
                "class SentimentDataset(Dataset):\n",
                "    def __init__(self, data, tokenizer, max_length=256, use_bert=False):\n",
                "        self.data = data\n",
                "        self.tokenizer = tokenizer\n",
                "        self.max_length = max_length\n",
                "        self.use_bert = use_bert\n",
                "    \n",
                "    def __len__(self):\n",
                "        return len(self.data)\n",
                "    \n",
                "    def __getitem__(self, idx):\n",
                "        item = self.data[idx]\n",
                "        text = item['text']\n",
                "        label = item['label']\n",
                "        \n",
                "        if self.use_bert:\n",
                "            # BERT tokenization\n",
                "            encoding = self.tokenizer(\n",
                "                text,\n",
                "                max_length=self.max_length,\n",
                "                padding='max_length',\n",
                "                truncation=True,\n",
                "                return_tensors='pt'\n",
                "            )\n",
                "            return {\n",
                "                'input_ids': encoding['input_ids'].squeeze(),\n",
                "                'attention_mask': encoding['attention_mask'].squeeze(),\n",
                "                'label': torch.tensor(label, dtype=torch.long)\n",
                "            }\n",
                "        else:\n",
                "            # Simple tokenization for LSTM\n",
                "            indices = self.tokenizer.encode(text, max_length=self.max_length)\n",
                "            return {\n",
                "                'input_ids': torch.tensor(indices, dtype=torch.long),\n",
                "                'label': torch.tensor(label, dtype=torch.long)\n",
                "            }\n",
                "\n",
                "# Create datasets for LSTM\n",
                "lstm_train_dataset = SentimentDataset(train_dataset, simple_tokenizer, max_length=256, use_bert=False)\n",
                "lstm_test_dataset = SentimentDataset(test_dataset, simple_tokenizer, max_length=256, use_bert=False)\n",
                "\n",
                "print(f\"‚úÖ LSTM datasets ready!\")\n",
                "print(f\"   Training samples: {len(lstm_train_dataset)}\")\n",
                "print(f\"   Test samples: {len(lstm_test_dataset)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Building the LSTM Model\n",
                "\n",
                "### LSTM Architecture:\n",
                "\n",
                "```\n",
                "Input Text\n",
                "    ‚Üì\n",
                "Embedding Layer (converts word IDs to vectors)\n",
                "    ‚Üì\n",
                "LSTM Layer 1 (processes sequence)\n",
                "    ‚Üì\n",
                "LSTM Layer 2 (learns higher-level patterns)\n",
                "    ‚Üì\n",
                "Fully Connected Layer\n",
                "    ‚Üì\n",
                "Output (Positive or Negative)\n",
                "```\n",
                "\n",
                "### How LSTM Reads:\n",
                "LSTM reads **one word at a time**, maintaining a \"memory\" of what it has seen so far."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class LSTMSentimentClassifier(nn.Module):\n",
                "    def __init__(self, vocab_size, embedding_dim=128, hidden_dim=256, num_layers=2, dropout=0.3):\n",
                "        super(LSTMSentimentClassifier, self).__init__()\n",
                "        \n",
                "        # Embedding layer: converts word indices to dense vectors\n",
                "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
                "        \n",
                "        # LSTM layers\n",
                "        self.lstm = nn.LSTM(\n",
                "            embedding_dim,\n",
                "            hidden_dim,\n",
                "            num_layers=num_layers,\n",
                "            batch_first=True,\n",
                "            dropout=dropout if num_layers > 1 else 0,\n",
                "            bidirectional=True  # Read both forward and backward\n",
                "        )\n",
                "        \n",
                "        # Fully connected layers\n",
                "        self.fc = nn.Sequential(\n",
                "            nn.Linear(hidden_dim * 2, 128),  # *2 because bidirectional\n",
                "            nn.ReLU(),\n",
                "            nn.Dropout(dropout),\n",
                "            nn.Linear(128, 2)  # 2 classes: positive/negative\n",
                "        )\n",
                "    \n",
                "    def forward(self, input_ids):\n",
                "        # Embed the input\n",
                "        embedded = self.embedding(input_ids)  # (batch, seq_len, embedding_dim)\n",
                "        \n",
                "        # Pass through LSTM\n",
                "        lstm_out, (hidden, cell) = self.lstm(embedded)\n",
                "        \n",
                "        # Use the last hidden state from both directions\n",
                "        # hidden shape: (num_layers * 2, batch, hidden_dim)\n",
                "        hidden_fwd = hidden[-2]  # Last layer, forward direction\n",
                "        hidden_bwd = hidden[-1]  # Last layer, backward direction\n",
                "        hidden_concat = torch.cat([hidden_fwd, hidden_bwd], dim=1)\n",
                "        \n",
                "        # Pass through fully connected layers\n",
                "        output = self.fc(hidden_concat)\n",
                "        \n",
                "        return output\n",
                "\n",
                "# Create LSTM model\n",
                "vocab_size = len(simple_tokenizer.word_to_idx)\n",
                "lstm_model = LSTMSentimentClassifier(vocab_size).to(device)\n",
                "\n",
                "# Count parameters\n",
                "lstm_params = sum(p.numel() for p in lstm_model.parameters())\n",
                "print(f\"\\nüìä LSTM Model:\")\n",
                "print(f\"   Parameters: {lstm_params:,}\")\n",
                "print(f\"   Vocabulary size: {vocab_size:,}\")\n",
                "print(f\"\\n{lstm_model}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Building the Transformer Model\n",
                "\n",
                "### Transformer Architecture:\n",
                "\n",
                "```\n",
                "Input Text\n",
                "    ‚Üì\n",
                "Embedding + Positional Encoding\n",
                "    ‚Üì\n",
                "Multi-Head Self-Attention (all words look at each other)\n",
                "    ‚Üì\n",
                "Feed-Forward Network\n",
                "    ‚Üì\n",
                "Classification Head\n",
                "    ‚Üì\n",
                "Output (Positive or Negative)\n",
                "```\n",
                "\n",
                "### How Transformer Reads:\n",
                "Transformer reads **all words at once**, with each word \"attending\" to all other words to understand context."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class TransformerSentimentClassifier(nn.Module):\n",
                "    def __init__(self, vocab_size, embedding_dim=128, num_heads=8, num_layers=2, dropout=0.3, max_length=256):\n",
                "        super(TransformerSentimentClassifier, self).__init__()\n",
                "        \n",
                "        # Embedding layer\n",
                "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
                "        \n",
                "        # Positional encoding (helps model understand word order)\n",
                "        self.positional_encoding = nn.Parameter(\n",
                "            self._create_positional_encoding(max_length, embedding_dim),\n",
                "            requires_grad=False\n",
                "        )\n",
                "        \n",
                "        # Transformer encoder layers\n",
                "        encoder_layer = nn.TransformerEncoderLayer(\n",
                "            d_model=embedding_dim,\n",
                "            nhead=num_heads,\n",
                "            dim_feedforward=embedding_dim * 4,\n",
                "            dropout=dropout,\n",
                "            batch_first=True\n",
                "        )\n",
                "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
                "        \n",
                "        # Classification head\n",
                "        self.fc = nn.Sequential(\n",
                "            nn.Linear(embedding_dim, 128),\n",
                "            nn.ReLU(),\n",
                "            nn.Dropout(dropout),\n",
                "            nn.Linear(128, 2)\n",
                "        )\n",
                "    \n",
                "    def _create_positional_encoding(self, max_length, embedding_dim):\n",
                "        \"\"\"Create sinusoidal positional encoding.\"\"\"\n",
                "        position = torch.arange(max_length).unsqueeze(1)\n",
                "        div_term = torch.exp(torch.arange(0, embedding_dim, 2) * (-np.log(10000.0) / embedding_dim))\n",
                "        \n",
                "        pe = torch.zeros(max_length, embedding_dim)\n",
                "        pe[:, 0::2] = torch.sin(position * div_term)\n",
                "        pe[:, 1::2] = torch.cos(position * div_term)\n",
                "        \n",
                "        return pe.unsqueeze(0)  # Add batch dimension\n",
                "    \n",
                "    def forward(self, input_ids, attention_mask=None):\n",
                "        # Embed the input\n",
                "        embedded = self.embedding(input_ids)  # (batch, seq_len, embedding_dim)\n",
                "        \n",
                "        # Add positional encoding\n",
                "        seq_len = embedded.size(1)\n",
                "        embedded = embedded + self.positional_encoding[:, :seq_len, :]\n",
                "        \n",
                "        # Create padding mask for transformer\n",
                "        if attention_mask is None:\n",
                "            attention_mask = (input_ids != 0)  # Mask padding tokens\n",
                "        \n",
                "        # Invert mask (Transformer expects True for positions to mask)\n",
                "        padding_mask = ~attention_mask\n",
                "        \n",
                "        # Pass through transformer\n",
                "        transformer_out = self.transformer(embedded, src_key_padding_mask=padding_mask)\n",
                "        \n",
                "        # Use mean pooling over sequence\n",
                "        # Mask out padding tokens before averaging\n",
                "        mask_expanded = attention_mask.unsqueeze(-1).expand(transformer_out.size()).float()\n",
                "        sum_embeddings = torch.sum(transformer_out * mask_expanded, dim=1)\n",
                "        sum_mask = torch.clamp(mask_expanded.sum(dim=1), min=1e-9)\n",
                "        pooled = sum_embeddings / sum_mask\n",
                "        \n",
                "        # Classification\n",
                "        output = self.fc(pooled)\n",
                "        \n",
                "        return output\n",
                "\n",
                "# Create Transformer model\n",
                "transformer_model = TransformerSentimentClassifier(vocab_size).to(device)\n",
                "\n",
                "# Count parameters\n",
                "transformer_params = sum(p.numel() for p in transformer_model.parameters())\n",
                "print(f\"\\nüìä Transformer Model:\")\n",
                "print(f\"   Parameters: {transformer_params:,}\")\n",
                "print(f\"   Vocabulary size: {vocab_size:,}\")\n",
                "print(f\"\\n{transformer_model}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Training Function\n",
                "\n",
                "We'll create a unified training function that works for both models."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def train_model(model, train_dataset, test_dataset, model_name, epochs=3, batch_size=32, lr=0.001):\n",
                "    \"\"\"\n",
                "    Train and evaluate a model.\n",
                "    \n",
                "    Args:\n",
                "        model: The model to train\n",
                "        train_dataset: Training dataset\n",
                "        test_dataset: Test dataset\n",
                "        model_name: Name for logging\n",
                "        epochs: Number of training epochs\n",
                "        batch_size: Batch size\n",
                "        lr: Learning rate\n",
                "    \n",
                "    Returns:\n",
                "        Dictionary with training history and metrics\n",
                "    \"\"\"\n",
                "    # Create data loaders\n",
                "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
                "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
                "    \n",
                "    # Loss and optimizer\n",
                "    criterion = nn.CrossEntropyLoss()\n",
                "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
                "    \n",
                "    # Training history\n",
                "    history = {\n",
                "        'train_loss': [],\n",
                "        'train_acc': [],\n",
                "        'test_loss': [],\n",
                "        'test_acc': [],\n",
                "        'epoch_times': []\n",
                "    }\n",
                "    \n",
                "    print(f\"\\n{'='*60}\")\n",
                "    print(f\"Training {model_name}\")\n",
                "    print(f\"{'='*60}\")\n",
                "    \n",
                "    for epoch in range(epochs):\n",
                "        epoch_start = time.time()\n",
                "        \n",
                "        # Training phase\n",
                "        model.train()\n",
                "        train_loss = 0\n",
                "        train_correct = 0\n",
                "        train_total = 0\n",
                "        \n",
                "        train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} [Train]\")\n",
                "        for batch in train_pbar:\n",
                "            input_ids = batch['input_ids'].to(device)\n",
                "            labels = batch['label'].to(device)\n",
                "            \n",
                "            # Forward pass\n",
                "            optimizer.zero_grad()\n",
                "            \n",
                "            # Handle different model inputs\n",
                "            if 'attention_mask' in batch:\n",
                "                attention_mask = batch['attention_mask'].to(device)\n",
                "                outputs = model(input_ids, attention_mask)\n",
                "            else:\n",
                "                outputs = model(input_ids)\n",
                "            \n",
                "            loss = criterion(outputs, labels)\n",
                "            \n",
                "            # Backward pass\n",
                "            loss.backward()\n",
                "            optimizer.step()\n",
                "            \n",
                "            # Statistics\n",
                "            train_loss += loss.item()\n",
                "            _, predicted = torch.max(outputs, 1)\n",
                "            train_total += labels.size(0)\n",
                "            train_correct += (predicted == labels).sum().item()\n",
                "            \n",
                "            # Update progress bar\n",
                "            train_pbar.set_postfix({\n",
                "                'loss': f\"{loss.item():.4f}\",\n",
                "                'acc': f\"{100 * train_correct / train_total:.2f}%\"\n",
                "            })\n",
                "        \n",
                "        # Evaluation phase\n",
                "        model.eval()\n",
                "        test_loss = 0\n",
                "        test_correct = 0\n",
                "        test_total = 0\n",
                "        \n",
                "        with torch.no_grad():\n",
                "            test_pbar = tqdm(test_loader, desc=f\"Epoch {epoch+1}/{epochs} [Test]\")\n",
                "            for batch in test_pbar:\n",
                "                input_ids = batch['input_ids'].to(device)\n",
                "                labels = batch['label'].to(device)\n",
                "                \n",
                "                # Forward pass\n",
                "                if 'attention_mask' in batch:\n",
                "                    attention_mask = batch['attention_mask'].to(device)\n",
                "                    outputs = model(input_ids, attention_mask)\n",
                "                else:\n",
                "                    outputs = model(input_ids)\n",
                "                \n",
                "                loss = criterion(outputs, labels)\n",
                "                \n",
                "                # Statistics\n",
                "                test_loss += loss.item()\n",
                "                _, predicted = torch.max(outputs, 1)\n",
                "                test_total += labels.size(0)\n",
                "                test_correct += (predicted == labels).sum().item()\n",
                "        \n",
                "        # Calculate metrics\n",
                "        epoch_time = time.time() - epoch_start\n",
                "        train_loss = train_loss / len(train_loader)\n",
                "        train_acc = 100 * train_correct / train_total\n",
                "        test_loss = test_loss / len(test_loader)\n",
                "        test_acc = 100 * test_correct / test_total\n",
                "        \n",
                "        # Save history\n",
                "        history['train_loss'].append(train_loss)\n",
                "        history['train_acc'].append(train_acc)\n",
                "        history['test_loss'].append(test_loss)\n",
                "        history['test_acc'].append(test_acc)\n",
                "        history['epoch_times'].append(epoch_time)\n",
                "        \n",
                "        # Print epoch summary\n",
                "        print(f\"\\nEpoch {epoch+1} Summary:\")\n",
                "        print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
                "        print(f\"  Test Loss:  {test_loss:.4f} | Test Acc:  {test_acc:.2f}%\")\n",
                "        print(f\"  Time: {epoch_time:.2f}s\")\n",
                "    \n",
                "    return history\n",
                "\n",
                "print(\"‚úÖ Training function ready!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Train LSTM Model\n",
                "\n",
                "Let's train the LSTM model and see how it performs!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train LSTM\n",
                "lstm_history = train_model(\n",
                "    model=lstm_model,\n",
                "    train_dataset=lstm_train_dataset,\n",
                "    test_dataset=lstm_test_dataset,\n",
                "    model_name=\"LSTM\",\n",
                "    epochs=3,\n",
                "    batch_size=32,\n",
                "    lr=0.001\n",
                ")\n",
                "\n",
                "print(\"\\n‚úÖ LSTM training complete!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Train Transformer Model\n",
                "\n",
                "Now let's train the Transformer model with the same data!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train Transformer\n",
                "transformer_history = train_model(\n",
                "    model=transformer_model,\n",
                "    train_dataset=lstm_train_dataset,  # Same dataset\n",
                "    test_dataset=lstm_test_dataset,\n",
                "    model_name=\"Transformer\",\n",
                "    epochs=3,\n",
                "    batch_size=32,\n",
                "    lr=0.001\n",
                ")\n",
                "\n",
                "print(\"\\n‚úÖ Transformer training complete!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Compare Results\n",
                "\n",
                "Let's visualize and compare the performance of both models!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot comparison\n",
                "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
                "\n",
                "epochs_range = range(1, len(lstm_history['train_loss']) + 1)\n",
                "\n",
                "# Plot 1: Training Loss\n",
                "axes[0, 0].plot(epochs_range, lstm_history['train_loss'], 'b-o', label='LSTM', linewidth=2)\n",
                "axes[0, 0].plot(epochs_range, transformer_history['train_loss'], 'r-s', label='Transformer', linewidth=2)\n",
                "axes[0, 0].set_xlabel('Epoch', fontsize=12)\n",
                "axes[0, 0].set_ylabel('Loss', fontsize=12)\n",
                "axes[0, 0].set_title('Training Loss Comparison', fontsize=14, fontweight='bold')\n",
                "axes[0, 0].legend(fontsize=11)\n",
                "axes[0, 0].grid(True, alpha=0.3)\n",
                "\n",
                "# Plot 2: Test Loss\n",
                "axes[0, 1].plot(epochs_range, lstm_history['test_loss'], 'b-o', label='LSTM', linewidth=2)\n",
                "axes[0, 1].plot(epochs_range, transformer_history['test_loss'], 'r-s', label='Transformer', linewidth=2)\n",
                "axes[0, 1].set_xlabel('Epoch', fontsize=12)\n",
                "axes[0, 1].set_ylabel('Loss', fontsize=12)\n",
                "axes[0, 1].set_title('Test Loss Comparison', fontsize=14, fontweight='bold')\n",
                "axes[0, 1].legend(fontsize=11)\n",
                "axes[0, 1].grid(True, alpha=0.3)\n",
                "\n",
                "# Plot 3: Training Accuracy\n",
                "axes[1, 0].plot(epochs_range, lstm_history['train_acc'], 'b-o', label='LSTM', linewidth=2)\n",
                "axes[1, 0].plot(epochs_range, transformer_history['train_acc'], 'r-s', label='Transformer', linewidth=2)\n",
                "axes[1, 0].set_xlabel('Epoch', fontsize=12)\n",
                "axes[1, 0].set_ylabel('Accuracy (%)', fontsize=12)\n",
                "axes[1, 0].set_title('Training Accuracy Comparison', fontsize=14, fontweight='bold')\n",
                "axes[1, 0].legend(fontsize=11)\n",
                "axes[1, 0].grid(True, alpha=0.3)\n",
                "\n",
                "# Plot 4: Test Accuracy\n",
                "axes[1, 1].plot(epochs_range, lstm_history['test_acc'], 'b-o', label='LSTM', linewidth=2)\n",
                "axes[1, 1].plot(epochs_range, transformer_history['test_acc'], 'r-s', label='Transformer', linewidth=2)\n",
                "axes[1, 1].set_xlabel('Epoch', fontsize=12)\n",
                "axes[1, 1].set_ylabel('Accuracy (%)', fontsize=12)\n",
                "axes[1, 1].set_title('Test Accuracy Comparison', fontsize=14, fontweight='bold')\n",
                "axes[1, 1].legend(fontsize=11)\n",
                "axes[1, 1].grid(True, alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "# Print final comparison\n",
                "print(f\"\\n{'='*60}\")\n",
                "print(\"FINAL RESULTS COMPARISON\")\n",
                "print(f\"{'='*60}\")\n",
                "print(f\"\\n{'Model':<15} {'Test Accuracy':<15} {'Avg Time/Epoch'}\")\n",
                "print(f\"{'-'*50}\")\n",
                "print(f\"{'LSTM':<15} {lstm_history['test_acc'][-1]:<14.2f}% {np.mean(lstm_history['epoch_times']):.2f}s\")\n",
                "print(f\"{'Transformer':<15} {transformer_history['test_acc'][-1]:<14.2f}% {np.mean(transformer_history['epoch_times']):.2f}s\")\n",
                "print(f\"\\n{'='*60}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. Test on Custom Examples\n",
                "\n",
                "Let's test both models on some custom movie reviews!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def predict_sentiment(text, model, tokenizer, model_name):\n",
                "    \"\"\"\n",
                "    Predict sentiment for a given text.\n",
                "    \"\"\"\n",
                "    model.eval()\n",
                "    \n",
                "    # Tokenize\n",
                "    indices = tokenizer.encode(text, max_length=256)\n",
                "    input_ids = torch.tensor([indices], dtype=torch.long).to(device)\n",
                "    \n",
                "    # Predict\n",
                "    with torch.no_grad():\n",
                "        outputs = model(input_ids)\n",
                "        probabilities = F.softmax(outputs, dim=1)\n",
                "        predicted_class = torch.argmax(probabilities, dim=1).item()\n",
                "        confidence = probabilities[0][predicted_class].item()\n",
                "    \n",
                "    sentiment = \"Positive üòä\" if predicted_class == 1 else \"Negative üòû\"\n",
                "    \n",
                "    return sentiment, confidence\n",
                "\n",
                "# Test examples\n",
                "test_reviews = [\n",
                "    \"This movie was absolutely fantastic! Best film I've seen all year.\",\n",
                "    \"Terrible waste of time. I want my money back.\",\n",
                "    \"The acting was great but the plot was confusing.\",\n",
                "    \"An emotional masterpiece that will stay with you forever.\",\n",
                "    \"Boring and predictable. Fell asleep halfway through.\"\n",
                "]\n",
                "\n",
                "print(f\"\\n{'='*80}\")\n",
                "print(\"TESTING ON CUSTOM REVIEWS\")\n",
                "print(f\"{'='*80}\")\n",
                "\n",
                "for i, review in enumerate(test_reviews, 1):\n",
                "    print(f\"\\n{'-'*80}\")\n",
                "    print(f\"Review {i}: {review}\")\n",
                "    print(f\"{'-'*80}\")\n",
                "    \n",
                "    # LSTM prediction\n",
                "    lstm_sentiment, lstm_conf = predict_sentiment(review, lstm_model, simple_tokenizer, \"LSTM\")\n",
                "    print(f\"LSTM:        {lstm_sentiment:<15} (Confidence: {lstm_conf:.2%})\")\n",
                "    \n",
                "    # Transformer prediction\n",
                "    trans_sentiment, trans_conf = predict_sentiment(review, transformer_model, simple_tokenizer, \"Transformer\")\n",
                "    print(f\"Transformer: {trans_sentiment:<15} (Confidence: {trans_conf:.2%})\")\n",
                "\n",
                "print(f\"\\n{'='*80}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 11. Understanding the Differences\n",
                "\n",
                "### How Each Model Reads Text:\n",
                "\n",
                "#### LSTM (Sequential Processing):\n",
                "```\n",
                "Review: \"This movie was absolutely fantastic!\"\n",
                "\n",
                "Step 1: Read \"This\"        ‚Üí Hidden state h‚ÇÅ\n",
                "Step 2: Read \"movie\"       ‚Üí Update to h‚ÇÇ (remembers \"This\")\n",
                "Step 3: Read \"was\"         ‚Üí Update to h‚ÇÉ (remembers \"This movie\")\n",
                "Step 4: Read \"absolutely\"  ‚Üí Update to h‚ÇÑ\n",
                "Step 5: Read \"fantastic\"   ‚Üí Update to h‚ÇÖ\n",
                "Step 6: Final prediction based on h‚ÇÖ\n",
                "```\n",
                "\n",
                "**Problem**: By the time LSTM reaches \"fantastic\", it might have partially \"forgotten\" \"This\" due to the long sequence.\n",
                "\n",
                "#### Transformer (Parallel Processing with Attention):\n",
                "```\n",
                "Review: \"This movie was absolutely fantastic!\"\n",
                "\n",
                "All words processed simultaneously!\n",
                "\n",
                "\"fantastic\" directly attends to:\n",
                "  - \"This\"      (0.05) - low attention\n",
                "  - \"movie\"     (0.60) - high attention! (what is fantastic?)\n",
                "  - \"was\"       (0.10)\n",
                "  - \"absolutely\" (0.20) - moderate attention (intensifier)\n",
                "  - \"fantastic\" (0.05) - itself\n",
                "\n",
                "Direct connections between ALL words!\n",
                "```\n",
                "\n",
                "**Advantage**: Every word can directly \"look at\" every other word, capturing relationships better."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 12. Visualizing Attention (Transformer)\n",
                "\n",
                "Let's visualize what the Transformer is \"paying attention to\"!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def visualize_attention_simple(text, model, tokenizer):\n",
                "    \"\"\"\n",
                "    Visualize which words the model focuses on.\n",
                "    This is a simplified visualization showing word importance.\n",
                "    \"\"\"\n",
                "    model.eval()\n",
                "    \n",
                "    # Tokenize\n",
                "    words = text.split()\n",
                "    indices = tokenizer.encode(text, max_length=256)\n",
                "    input_ids = torch.tensor([indices], dtype=torch.long).to(device)\n",
                "    \n",
                "    # Get embeddings and compute importance\n",
                "    with torch.no_grad():\n",
                "        # Get embedding\n",
                "        embedded = model.embedding(input_ids)\n",
                "        \n",
                "        # Simple importance: L2 norm of embeddings after transformer\n",
                "        # (This is a simplification - real attention is more complex)\n",
                "        importance = torch.norm(embedded, dim=2).squeeze().cpu().numpy()\n",
                "        \n",
                "        # Normalize to 0-1\n",
                "        importance = importance[:len(words)]\n",
                "        importance = (importance - importance.min()) / (importance.max() - importance.min() + 1e-10)\n",
                "    \n",
                "    # Visualize\n",
                "    plt.figure(figsize=(12, 4))\n",
                "    colors = plt.cm.Reds(importance)\n",
                "    \n",
                "    plt.bar(range(len(words)), importance, color=colors)\n",
                "    plt.xticks(range(len(words)), words, rotation=45, ha='right')\n",
                "    plt.ylabel('Importance', fontsize=12)\n",
                "    plt.title('Word Importance in Transformer Model', fontsize=14, fontweight='bold')\n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "    \n",
                "    # Print top words\n",
                "    word_importance = list(zip(words, importance))\n",
                "    word_importance.sort(key=lambda x: x[1], reverse=True)\n",
                "    \n",
                "    print(\"\\nTop 5 Most Important Words:\")\n",
                "    for word, imp in word_importance[:5]:\n",
                "        print(f\"  {word:<15} {imp:.3f}\")\n",
                "\n",
                "# Visualize attention for a sample review\n",
                "sample_review = \"This movie was absolutely fantastic and entertaining\"\n",
                "print(f\"Analyzing: '{sample_review}'\\n\")\n",
                "visualize_attention_simple(sample_review, transformer_model, simple_tokenizer)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 13. Key Takeaways\n",
                "\n",
                "### Performance Comparison:\n",
                "\n",
                "| Aspect | LSTM | Transformer |\n",
                "|--------|------|-------------|\n",
                "| **Processing** | Sequential (one word at a time) | Parallel (all words together) |\n",
                "| **Speed** | Slower (can't parallelize) | Faster (GPU-friendly) |\n",
                "| **Long-range dependencies** | Struggles with very long sequences | Excellent - direct connections |\n",
                "| **Memory** | Hidden state can \"forget\" | Attention to all words |\n",
                "| **Training** | Slower | Faster with GPU |\n",
                "| **Accuracy** | Good | Usually better |\n",
                "\n",
                "### Why Transformers Win:\n",
                "\n",
                "1. **Parallel Processing**: Can process all words simultaneously ‚Üí faster training\n",
                "2. **Direct Connections**: Every word can attend to every other word ‚Üí better understanding\n",
                "3. **No Forgetting**: No sequential bottleneck ‚Üí better long-range dependencies\n",
                "4. **Scalability**: Scales better with more data and compute\n",
                "\n",
                "### When to Use Each:\n",
                "\n",
                "**Use LSTM when:**\n",
                "- You have limited computational resources\n",
                "- Working with very long sequences (lower memory)\n",
                "- Sequential nature is important (e.g., time series)\n",
                "\n",
                "**Use Transformer when:**\n",
                "- You need best performance\n",
                "- Have GPU resources\n",
                "- Working with natural language\n",
                "- Need to understand complex relationships"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 14. Exercises for Students\n",
                "\n",
                "### Exercise 1: Experiment with Hyperparameters\n",
                "Try changing:\n",
                "- Number of LSTM/Transformer layers\n",
                "- Embedding dimensions\n",
                "- Learning rate\n",
                "- Batch size\n",
                "\n",
                "Which changes improve performance the most?\n",
                "\n",
                "### Exercise 2: Test on Different Texts\n",
                "Create your own movie reviews and test both models. Try:\n",
                "- Very short reviews (5 words)\n",
                "- Very long reviews (100+ words)\n",
                "- Mixed sentiment reviews\n",
                "\n",
                "Which model handles each case better?\n",
                "\n",
                "### Exercise 3: Analyze Errors\n",
                "Find examples where:\n",
                "- Both models are wrong\n",
                "- LSTM is right but Transformer is wrong\n",
                "- Transformer is right but LSTM is wrong\n",
                "\n",
                "What patterns do you notice?\n",
                "\n",
                "### Exercise 4: Add More Data\n",
                "Increase the training data size from 5000 to 10000 or 20000 samples.\n",
                "Does this help one model more than the other?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Space for your experiments!\n",
                "\n",
                "# Exercise 1: Try different hyperparameters\n",
                "# TODO: Modify model parameters and retrain\n",
                "\n",
                "# Exercise 2: Test your own reviews\n",
                "my_reviews = [\n",
                "    # Add your own reviews here!\n",
                "]\n",
                "\n",
                "# Exercise 3: Analyze predictions\n",
                "# TODO: Compare predictions and find patterns\n",
                "\n",
                "# Exercise 4: Train with more data\n",
                "# TODO: Load more training samples and compare results"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 15. Summary\n",
                "\n",
                "### What We Learned:\n",
                "\n",
                "‚úÖ **LSTM Models**:\n",
                "- Process text sequentially (one word at a time)\n",
                "- Use hidden states to remember previous words\n",
                "- Can struggle with long-range dependencies\n",
                "- Good for sequential data with limited resources\n",
                "\n",
                "‚úÖ **Transformer Models**:\n",
                "- Process all words in parallel\n",
                "- Use attention to connect every word to every other word\n",
                "- Better at understanding context and relationships\n",
                "- State-of-the-art for most NLP tasks\n",
                "\n",
                "‚úÖ **Key Insight**:\n",
                "Transformers generally perform better because they can:\n",
                "1. See all words at once (parallel processing)\n",
                "2. Directly connect any two words (attention mechanism)\n",
                "3. Train faster on GPUs\n",
                "4. Scale better with more data\n",
                "\n",
                "### Next Steps:\n",
                "\n",
                "1. **Try pre-trained models**: Use BERT, RoBERTa, or GPT for even better results\n",
                "2. **Explore attention visualization**: Understand what transformers really \"see\"\n",
                "3. **Apply to other tasks**: Question answering, translation, summarization\n",
                "4. **Learn about modern architectures**: GPT-4, BERT, T5, etc.\n",
                "\n",
                "---\n",
                "\n",
                "**üéâ Congratulations!** You now understand the fundamental difference between sequential (LSTM) and attention-based (Transformer) models!\n",
                "\n",
                "*Remember: The future of NLP is built on Transformers, but understanding LSTMs helps you appreciate why Transformers are so powerful!*"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
